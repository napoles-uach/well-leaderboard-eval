[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "well-leaderboard-eval"
version = "0.1.0"
description = "Reusable evaluator for The Well leaderboard metrics (v1.1.0-compatible)."
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}

dependencies = [
  "numpy>=1.24",
  "torch>=2.1",
  "einops>=0.7",
  "tqdm>=4.66",
  "hydra-core>=1.3",
  "datasets>=2.18",
  "huggingface_hub>=0.23",
  "timm>=0.9",
  "torch_harmonics>=0.6",
  "safetensors>=0.4",
  "neuraloperator==0.3.0",
  # Pin The Well benchmark code to v1.1.0
  "the-well-benchmark @ git+https://github.com/PolymathicAI/the_well.git@v1.1.0",
]

[project.optional-dependencies]
modal = ["modal>=0.64"]
dev = ["pytest>=8.0", "ruff>=0.5"]

[project.scripts]
well-eval = "well_leaderboard_eval.cli:main"

[tool.ruff]
line-length = 100
